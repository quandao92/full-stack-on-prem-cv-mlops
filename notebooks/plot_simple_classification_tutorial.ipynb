{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Data Validation in 5 Minutes {#vision_simple_classification_tutorial}\n",
    "==================================\n",
    "\n",
    "Deepchecks Vision is built to validate your data and model, however\n",
    "complex your model and data may be. That being said, sometime there is\n",
    "no need to write a full-blown\n",
    "`classification task <vision__classification_tutorial>`{.interpreted-text\n",
    "role=\"ref\"},\n",
    "`object detection task <vision__detection_tutorial>`{.interpreted-text\n",
    "role=\"ref\"} or\n",
    "`semantic segmentation task <vision__segmentation_tutorial>`{.interpreted-text\n",
    "role=\"ref\"}. In the case of a simple classification task, there are\n",
    "quite a few checks that can be run writing only a few lines of code. In\n",
    "this tutorial, we will show you how to run all checks that do not\n",
    "require a model on a simple classification task.\n",
    "\n",
    "This is ideal, for example, when receiving a new dataset for a\n",
    "classification task. Running these checks on the dataset before even\n",
    "starting with training will give you a quick idea of how the dataset\n",
    "looks like and what potential issues it contains.\n",
    "\n",
    "``` {.bash}\n",
    "# Before we start, if you don't have deepchecks vision package installed yet, run:\n",
    "import sys\n",
    "!{sys.executable} -m pip install \"deepchecks[vision]\" --quiet --upgrade # --user\n",
    "\n",
    "# or install using pip from your python environment\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the Data\n",
    "====================\n",
    "\n",
    "For this example we\\'ll use a small sample of the RGB [EuroSAT\n",
    "dataset](https://github.com/phelber/eurosat#). EuroSAT dataset is based\n",
    "on Sentinel-2 satellite images covering 13 spectral bands and consisting\n",
    "of 10 classes with 27000 labeled and geo-referenced samples.\n",
    "\n",
    "Citations:\n",
    "\n",
    "\\[1\\] Eurosat: A novel dataset and deep learning benchmark for land use\n",
    "and land cover classification. Patrick Helber, Benjamin Bischke, Andreas\n",
    "Dengel, Damian Borth. IEEE Journal of Selected Topics in Applied Earth\n",
    "Observations and Remote Sensing, 2019.\n",
    "\n",
    "\\[2\\] Introducing EuroSAT: A Novel Dataset and Deep Learning Benchmark\n",
    "for Land Use and Land Cover Classification. Patrick Helber, Benjamin\n",
    "Bischke, Andreas Dengel. 2018 IEEE International Geoscience and Remote\n",
    "Sensing Symposium, 2018.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "url = 'https://figshare.com/ndownloader/files/34912884'\n",
    "urllib.request.urlretrieve(url, 'EuroSAT_data.zip')\n",
    "\n",
    "with zipfile.ZipFile('EuroSAT_data.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('EuroSAT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a Simple Classification Dataset\n",
    "=======================================\n",
    "\n",
    "A simple classification dataset is an image dataset structured in the\n",
    "following way:\n",
    "\n",
    "> -   \n",
    ">\n",
    ">     root/\n",
    ">\n",
    ">     :   -   \n",
    ">\n",
    ">             train/\n",
    ">\n",
    ">             :   -   \n",
    ">\n",
    ">                     class1/\n",
    ">\n",
    ">                     :   image1.jpeg\n",
    ">\n",
    ">         -   \n",
    ">\n",
    ">             test/\n",
    ">\n",
    ">             :   -   \n",
    ">\n",
    ">                     class1/\n",
    ">\n",
    ">                     :   image1.jpeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Note that the following function requires torchvision>=0.11.3 to be installed\n",
    "from deepchecks.vision import classification_dataset_from_directory\n",
    "\n",
    "train_ds, test_ds = classification_dataset_from_directory(\n",
    "    root='./EuroSAT/euroSAT/', object_type='VisionData', image_extension='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = classification_dataset_from_directory(\n",
    "    root='/home/ariya/workspace/datasets/animals10-dvc/images', object_type='VisionData',\n",
    "    image_extension='jpeg'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Deepchecks\\' `train_test_validation` suite\n",
    "==================================================\n",
    "\n",
    "That\\'s it, we have just defined the classification data object and are\n",
    "ready can run the different deepchecks suites and checks. Here we will\n",
    "demonstrate how to run train\\_test\\_validation suite:\n",
    "\n",
    "for additional information on the different suites and checks available\n",
    "see our `Vision Checks <vision__checks_gallery>`{.interpreted-text\n",
    "role=\"ref\"} gallery.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.vision.suites import train_test_validation\n",
    "\n",
    "suite = train_test_validation()\n",
    "result = suite.run(train_ds, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the Results\n",
    "=====================\n",
    "\n",
    "The results can be saved as an HTML file with the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output (1).html'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.save_as_html('output.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, if working inside a notebook, the output can be displayed directly\n",
    "by simply printing the result object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc5b4bafb9e442eb2ffb28b6e108952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HTML(value='\\n<h1 id=\"summary_ZBIKCIDKWNNHJ7XVG0FN9XUY4\">Train Test Validat…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the Results\n",
    "=========================\n",
    "\n",
    "Looking at the results we see one check whose condition has failed:\n",
    "Feature Label Correlation.\n",
    "\n",
    "The `vision__property_label_correlation_change`{.interpreted-text\n",
    "role=\"ref\"} check computes various\n",
    "`image properties <vision__properties_guide>`{.interpreted-text\n",
    "role=\"ref\"} and checks if the image label can be inferred using a simple\n",
    "model (for example, a Classification Tree) using the property values.\n",
    "The ability to predict the label using these properties is measured by\n",
    "the Predictive Power Score (PPS) and this measure is compared between\n",
    "the training and test dataset. In this case, the condition alerts us to\n",
    "the fact that the PPS for the \\\"RMS Contrast\\\" property was\n",
    "significantly higher in the training dataset than in the test dataset.\n",
    "\n",
    "We\\'ll show the relevant plot again for ease of discussion:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e734da9bb248b9a90a97201a3a3737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Property Label Correlation Change</b></h4>'), HTML(value='<p>    Return the …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "check_idx = np.where([result.results[i].check.name() == 'Property Label Correlation Change'\n",
    "                      for i in range(len(result.results))])[0][0]\n",
    "result.results[check_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the plot dedicated to the PPS of the property RMS\n",
    "Contrast, which measures the contrast in the image by calculating the\n",
    "grayscale standard deviation of the image. This plot shows us that\n",
    "specifically for the classes \\\"Forest\\\" and \\\"SeaLake\\\" (the same\n",
    "culprits from the Similar Image Leakage condition), the contrast is a\n",
    "great predictor, but only in the training data! This means we have a\n",
    "critical problem - or model may learn to classify these classes using\n",
    "only the contrast, without actually learning anything about the image\n",
    "content. We can now go on and fix this issue (perhaps by adding train\n",
    "augmentations, or enriching our training set), before we have even\n",
    "trained a model on this task.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer-viz-dl",
   "language": "python",
   "name": "computer-viz-dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
